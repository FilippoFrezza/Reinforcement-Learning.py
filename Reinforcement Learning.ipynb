{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Cartpole Problem\n",
    "\n",
    "- Giving credit where credit is due: https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration vs Exploitation :\n",
    "\n",
    "These are two frameworks for solving Reinforcement Learning problems. \n",
    "\n",
    "To make the explanation easier just immagine you find yourself in Vegas, you find yourself in front of many, many slot machines and in order to maximise your return you can either sit down in front of one machine and one machine only and pull the lever all day long. This is known as __PURE EXPLOITATION APPROACH__.\n",
    "If you do the exact opposite, hence pull the level of every slot machine in the casino one after the other, you are going for the __PURE EXPLORATION APPROACH__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision process :\n",
    "\n",
    "This is a mathematical framework for defining solutions in a reinforcement learning task. \n",
    "The following are the elements used to build this mathematical framework:\n",
    "- Set of states, S\n",
    "- Set of actions, A\n",
    "- Reward function, R\n",
    "- Policy, π\n",
    "- Value, V\n",
    "\n",
    "Taking an action A will make us move from starting state (S) to end state (S). For the actions we took, we will gain rewards (R) or be punished for negative moves. The sequence of decisions made will define our policy (π) where the final sum of rewards/punishments will define our value (V). \n",
    "\n",
    "Keeping this in mind, at any given time (t), we have to maximise all possible values of S:\n",
    "\n",
    "                                   E(Rt | πt St)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the relevant variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-11 12:54:58,233] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "#from gym import envs\n",
    "#print(envs.registry.all())\n",
    "# These two lines of code will show you all the pre - customized environments the gym library has to offer.\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple double hidden layer neural network model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,402\n",
      "Trainable params: 2,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(32)) # layer 1\n",
    "model.add(Dense(64)) # layer 2\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring and Compiling our agent :\n",
    "\n",
    "- Policy = Epsilon Greedy (greedy approach to solving the problem)\n",
    "- Memory = Sequential (as to store the result of actions we performed and the rewards we get for each action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy() \n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show time !\n",
    "\n",
    "For the sake of making your project as visual and clear as possible, you can make __visualize = True__ so that the training process is shown to the user, however, keep in mind that this slows down training quite a lot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "    9/5000: episode: 1, duration: 0.083s, episode steps: 9, steps per second: 109, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-2.882, 1.749], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   21/5000: episode: 2, duration: 0.006s, episode steps: 12, steps per second: 1902, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.084 [-3.008, 2.002], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   33/5000: episode: 3, duration: 0.007s, episode steps: 12, steps per second: 1653, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.089 [-2.561, 1.619], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   43/5000: episode: 4, duration: 0.009s, episode steps: 10, steps per second: 1141, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-3.116, 1.934], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   51/5000: episode: 5, duration: 0.005s, episode steps: 8, steps per second: 1600, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.563, 1.604], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   62/5000: episode: 6, duration: 0.006s, episode steps: 11, steps per second: 1861, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.118 [-2.829, 1.770], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   82/5000: episode: 7, duration: 0.010s, episode steps: 20, steps per second: 1910, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.059 [-2.512, 1.609], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dqn.fit(env, nb_steps=5000, visualize=False, verbose=2)\n",
    "except:\n",
    "    next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 9.000, steps: 9\n",
      "Episode 2: reward: 10.000, steps: 10\n",
      "Episode 3: reward: 20.000, steps: 20\n",
      "Episode 4: reward: 9.000, steps: 9\n",
      "Episode 5: reward: 9.000, steps: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1230cd8d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
